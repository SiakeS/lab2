{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4435c349-e610-4eb7-be11-e07c7353eb5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lag\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c47bd9b-57b4-4651-a076-d1673b2724cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "db_url = \"jdbc:sqlserver://lab2-sqlserver.database.windows.net:1433;database=lab2SQL\"\n",
    "db_properties = {\n",
    "    \"user\": \"lab2admin\",\n",
    "    \"password\": \"password1#\",\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f4ac416-f718-4a5c-9f9b-0e9ec4cf3e24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def daily_return_rate(stock_name, start_date, end_date):\n",
    "    # Charger les données depuis la table SQL\n",
    "    df = spark.read.format(\"jdbc\").option(\"url\", db_url).option(\"dbtable\", \"combined_stock_data\").option(\"user\", db_properties[\"user\"]).option(\"password\", db_properties[\"password\"]).load()\n",
    "\n",
    "    # Filtrer les données pour la période spécifiée et le nom du stock\n",
    "    filtered_df = df.filter((col(\"stock_name\") == stock_name) & (col(\"Date\").between(start_date, end_date)))\n",
    "\n",
    "    # Calculer le taux de rendement quotidien\n",
    "    return_df = filtered_df.withColumn(\"daily_return_rate\", (col(\"Close\") - lag(\"Close\").over(Window.orderBy(\"Date\"))) / lag(\"Close\").over(Window.orderBy(\"Date\")))\n",
    "\n",
    "    return return_df.select(\"Date\", \"stock_name\", \"daily_return_rate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49eaefdf-70fb-4bae-8a99-5d8b25e0fa62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def moving_average(stock_name, start_date, end_date, window_size):\n",
    "    # Charger les données depuis la table SQL\n",
    "    df = spark.read.format(\"jdbc\").option(\"url\", db_url).option(\"dbtable\", \"combined_stock_data\").option(\"user\", db_properties[\"user\"]).option(\"password\", db_properties[\"password\"]).load()\n",
    "\n",
    "    # Filtrer les données pour la période spécifiée et le nom du stock\n",
    "    filtered_df = df.filter((col(\"stock_name\") == stock_name) & (col(\"Date\").between(start_date, end_date)))\n",
    "\n",
    "    # Définir la fenêtre pour le calcul de la moyenne mobile\n",
    "    window_spec = Window().orderBy(\"Date\").rowsBetween(-window_size, 0)\n",
    "\n",
    "    # Calculer la moyenne mobile sur la colonne du prix d'ouverture\n",
    "    moving_avg_df = filtered_df.withColumn(\"moving_average\", avg(\"Open\").over(window_spec))\n",
    "    return moving_avg_df.select(\"Date\", \"stock_name\", \"Open\", \"moving_average\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87bb7892-edb5-4b16-82e4-ae518cea7c39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialisation de la session Spark\n",
    "spark = SparkSession.builder.appName(\"DatabricksActivity\").getOrCreate()\n",
    "\n",
    "# Appel des fonctions avec les paramètres appropriés\n",
    "result_daily_return = daily_return_rate(\"AMAZON\", \"2017-01-05\", \"2018-01-05\")\n",
    "result_moving_average = moving_average(\"AMAZON\", \"2017-01-05\", \"2018-01-05\", 5)\n",
    "\n",
    "# Affichage les résultats\n",
    "result_daily_return.show()\n",
    "result_moving_average.show()\n",
    "\n",
    "# Define the output file names\n",
    "output_file_daily_return = \"daily_return.csv\"\n",
    "output_file_moving_average = \"moving_average.csv\"\n",
    "\n",
    "# Define your Azure Storage account details\n",
    "storage_account_name = \"lab2dl\"\n",
    "container_name = \"output\"\n",
    "storage_account_key = \"sv44h7iPnAhb229nllmkA4e9/nJWBSKe2bNOmI6XDaJ4nyE3KmsbK4mwE2gYt3hfN3lSsAblSLuk+AStwpTadw==\"  \n",
    "\n",
    "# Set the Azure Storage account key in the Spark configuration\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", storage_account_key)\n",
    "\n",
    "try:\n",
    "    # Write DataFrames to Azure Storage in CSV format without using a specific folder\n",
    "    result_daily_return.write.csv(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{output_file_daily_return}\", header=True, mode=\"overwrite\")\n",
    "    result_moving_average.write.csv(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{output_file_moving_average}\", header=True, mode=\"overwrite\")\n",
    "    print(\"Data written successfully to Azure Storage.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lab2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
